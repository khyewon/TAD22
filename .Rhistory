}
facebook_2021_articles <- rbind_pages(pages_2021)
facebook_2021_articles$response.docs.lead_paragraph[1]
table(facebook_2021_articles$response.docs.print_section)
setwd("C:/Users/kevin/Documents/GitHub/TAD22/")
# load required libraries
library(quanteda)
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
library(quanteda.textmodels)
library(readtext)
library(dplyr)
#----------------------------------------
# 1 Supervised Learning: Naive Bayes     ---
#----------------------------------------
#source of data: https://www.kaggle.com/rmisra/news-category-dataset#
# News_Category_Dataset_v2.json
# load data
news_data <- readRDS("news_data.rds")
#----------------------------------------
# 1 Supervised Learning: Naive Bayes     ---
#----------------------------------------
#source of data: https://www.kaggle.com/rmisra/news-category-dataset#
# News_Category_Dataset_v2.json
# load data
news_data <- readRDS("news_data.rds")
news_samp <- filter(news_data, category %in% c("CRIME", "SPORTS"))
news_samp1 <- select(news_samp, headline, category)
news_samp2<-setNames(object = news_samp1, nm = c("text", "class"))
# get a sense of how the text looks
dim(news_samp2)
head(news_samp2$text[news_samp2$class == "CRIME"])
head(news_samp2$text[news_samp2$class == "SPORTS"])
# some pre-processing (the rest will let dfm do)
news_samp2$text <- gsub(pattern = "'", "", news_samp2$text)  # replace apostrophes
head(news_samp2$text[news_samp2$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp2$class))
# split sample into training & test sets
set.seed(1984L)
prop_train <- 0.8
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class, smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
# baseline --- This is important, to see how much our model beats a model that just picks the modal class
baseline_acc <- max(prop.table(table(test_set$class)))
# get confusion matrix
cmat <- table(test_set$class, predicted_class)
nb_acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
install.packages("quanteda.textmodels")
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
library(quanteda.textmodels)
# load required libraries
library(quanteda)
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
library(quanteda.textmodels)
library(readtext)
library(dplyr)
install.packages("readtext")
library(readtext)
library(dplyr)
#----------------------------------------
# 1 Supervised Learning: Naive Bayes     ---
#----------------------------------------
#source of data: https://www.kaggle.com/rmisra/news-category-dataset#
# News_Category_Dataset_v2.json
# load data
news_data <- readRDS("news_data.rds")
news_samp <- filter(news_data, category %in% c("CRIME", "SPORTS"))
news_samp1 <- select(news_samp, headline, category)
news_samp2<-setNames(object = news_samp1, nm = c("text", "class"))
# get a sense of how the text looks
dim(news_samp2)
head(news_samp2$text[news_samp2$class == "CRIME"])
head(news_samp2$text[news_samp2$class == "SPORTS"])
# some pre-processing (the rest will let dfm do)
news_samp2$text <- gsub(pattern = "'", "", news_samp2$text)  # replace apostrophes
head(news_samp2$text[news_samp2$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp2$class))
# split sample into training & test sets
set.seed(1984L)
prop_train <- 0.8
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class, smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
# baseline --- This is important, to see how much our model beats a model that just picks the modal class
baseline_acc <- max(prop.table(table(test_set$class)))
# get confusion matrix
cmat <- table(test_set$class, predicted_class)
nb_acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "uniform")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
# take a look at the most discriminant features (get some face validity)
posterior <- data.frame(feature = rownames(t(nb_model_sm$param)),
post_CRIME = t(nb_model_sm$param)[,1],
post_SPORTS = t(nb_model_sm$param)[,2])
head(arrange(posterior, -post_SPORTS))
head(arrange(posterior, -post_CRIME))
# what does smoothing do? More generally, reduces the "weight" place on new information (the likelihood) vis-a-vis the prior.
plot(nb_model$param[1,], nb_model_sm$param[1,], xlim = c(0,0.02), ylim = c(0,0.02), xlab="No Smooth", ylab="Smooth") + abline(a = 0, b = 1, col = "red")
setwd("C:/Users/kevin/Documents/GitHub/TAD22/")
# load required libraries
library(quanteda)
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
library(quanteda.textmodels)
library(readtext)
library(dplyr)
#----------------------------------------
# 1 Supervised Learning: Naive Bayes     ---
#----------------------------------------
#source of data: https://www.kaggle.com/rmisra/news-category-dataset#
# News_Category_Dataset_v2.json
# load data
news_data <- readRDS("news_data.rds")
# subset data and keep relevant variables
#filter-- restricts dataset, dropping rows
table(news_data$category)
news_samp <- filter(news_data, category %in% c("CRIME", "SPORTS"))
news_samp1 <- select(news_samp, headline, category)
news_samp2<-setNames(object = news_samp1, nm = c("text", "class"))
# get a sense of how the text looks
dim(news_samp2)
head(news_samp2$text[news_samp2$class == "CRIME"])
head(news_samp2$text[news_samp2$class == "SPORTS"])
# some pre-processing (the rest will let dfm do)
news_samp2$text <- gsub(pattern = "'", "", news_samp2$text)  # replace apostrophes
head(news_samp2$text[news_samp2$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp2$class))
# split sample into training & test sets
set.seed(1984L)
prop_train <- 0.8
nrow(news_samp2)
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
prop_train*length(ids)
ceiling(prop_train*length(ids))
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class, smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
predicted_class
# baseline --- This is important, to see how much our model beats a model that just picks the modal class
baseline_acc <- max(prop.table(table(test_set$class)))
baseline_acc
# get confusion matrix
cmat <- table(test_set$class, predicted_class)
nb_acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "uniform")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
# take a look at the most discriminant features (get some face validity)
posterior <- data.frame(feature = rownames(t(nb_model_sm$param)),
post_CRIME = t(nb_model_sm$param)[,1],
post_SPORTS = t(nb_model_sm$param)[,2])
head(arrange(posterior, -post_SPORTS))
head(arrange(posterior, -post_CRIME))
# what does smoothing do? More generally, reduces the "weight" place on new information (the likelihood) vis-a-vis the prior.
plot(nb_model$param[1,], nb_model_sm$param[1,], xlim = c(0,0.02), ylim = c(0,0.02), xlab="No Smooth", ylab="Smooth") + abline(a = 0, b = 1, col = "red")
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
prop_train <- 0.2
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "uniform")
prop_train <- 0.2
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "uniform")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
?textmodel_nb
# subset data and keep relevant variables
#filter-- restricts dataset, dropping rows
news_data$category
# subset data and keep relevant variables
#filter-- restricts dataset, dropping rows
table(news_data$category)
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD22/")
# load required libraries
library(quanteda)
#The textmodel_*() functions formerly in quanteda have
#now been moved to the quanteda.textmodels package.
library(quanteda.textmodels)
library(readtext)
library(dplyr)
#----------------------------------------
# 1 Supervised Learning: Naive Bayes     ---
#----------------------------------------
#source of data: https://www.kaggle.com/rmisra/news-category-dataset#
# News_Category_Dataset_v2.json
# load data
news_data <- readRDS("news_data.rds")
## BIG Dataset!
# subset data and keep relevant variables
#filter-- restricts dataset, dropping rows
news_samp <- filter(news_data, category %in% c("CRIME", "SPORTS"))
##select --- keeps only named columns
news_samp1 <- select(news_samp, headline, category)
##setNames --- rename the variables in our new dataset
news_samp2<-setNames(object = news_samp1, nm = c("text", "class"))
# get a sense of how the text looks
dim(news_samp2)
head(news_samp2$text[news_samp2$class == "CRIME"])
head(news_samp2$text[news_samp2$class == "SPORTS"])
# some pre-processing (the rest will let dfm do)
news_samp2$text <- gsub(pattern = "'", "", news_samp2$text)  # replace apostrophes
head(news_samp2$text[news_samp2$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp2$class))
# split sample into training & test sets
set.seed(1984L)
###what does this do??
prop_train <- 0.2
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# w/o smoothing ----------------
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class, smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
# baseline --- This is important, to see how much our model beats a model that just picks the modal class
baseline_acc <- max(prop.table(table(test_set$class)))
# get confusion matrix
cmat <- table(test_set$class, predicted_class)
nb_acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
# w smoothing ----------------
# train model on the training set using Laplace smoothing
##Recall what this does --- want to avoid having any zeroes, which happens if there's a novel word in the test set
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "uniform")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "docfreq")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
news_samp <- filter(news_data, category %in% c("RELIGION", "SCIENCE"))
##select --- keeps only named columns
news_samp1 <- select(news_samp, headline, category)
##setNames --- rename the variables in our new dataset
news_samp2<-setNames(object = news_samp1, nm = c("text", "class"))
# get a sense of how the text looks
dim(news_samp2)
head(news_samp2$text[news_samp2$class == "CRIME"])
head(news_samp2$text[news_samp2$class == "SPORTS"])
# some pre-processing (the rest will let dfm do)
news_samp2$text <- gsub(pattern = "'", "", news_samp2$text)  # replace apostrophes
head(news_samp2$text[news_samp2$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp2$class))
# split sample into training & test sets
set.seed(1984L)
###what does this do??
prop_train <- 0.2
###need to create a training set that's 80% of the data
ids <- 1:nrow(news_samp2)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
## the - sign is "not"
ids_test <- ids[-ids_train]
train_set <- news_samp2[ids_train,]
test_set <- news_samp2[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# how does this look?
as.matrix(train_dfm)[1:5,1:5]
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# w/o smoothing ----------------
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class, smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
# baseline --- This is important, to see how much our model beats a model that just picks the modal class
baseline_acc <- max(prop.table(table(test_set$class)))
# get confusion matrix
cmat <- table(test_set$class, predicted_class)
nb_acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
nb_precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
# w smoothing ----------------
# train model on the training set using Laplace smoothing
##Recall what this does --- want to avoid having any zeroes, which happens if there's a novel word in the test set
nb_model_sm <- textmodel_nb(train_dfm, train_set$class, smooth = 1, prior = "docfreq")
# evaluate on test set
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm) # accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,]) # recall = TP / (TP + FN)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2]) # precision = TP / (TP + FP)
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
