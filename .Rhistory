require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
#To access the **syllables** of a text, we use `syllables()`:
?nsyllable
#To access the **syllables** of a text, we use `syllables()`:
??nsyllable
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
quanteda::nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
require(quanteda)
nscrabble(c("cat", "quixotry", "zoo"))
quanteda::nscrabble(c("cat", "quixotry", "zoo"))
#We can analyze the **lexical diversity** of texts, using `lexdiv()` on a dfm:
summary(data_corpus_inaugural)
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
textstat_readability(myDfm, "R")
install.packages("quanteda.textstats")
require(quanteda.textstats)
#To access the **syllables** of a text, we use `syllables()`:
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
#To access the **syllables** of a text, we use `syllables()`:
quanteda.textstats::nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
textstat_readability(myDfm, "R")
?textstat_readability
mycorpus <- (corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_readability(mycorpus, "R")
textstat_readability(mycorpus)
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
readab
## Presidential Inaugural Address Corpus
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
#We can **identify documents and terms that are similar to one another**, using `similarity()`:
?dfm
stopwords("english")
# compute some document similarities
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
# compute some document similarities
textstat_simil(presDfm, presDfm["1985-Reagan"],  margin = "documents")
# compute some document similarities
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
textstat_simil(presDfm, presDfm[c("2009-Obama", "2013-Obama"),], margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
dfm_weight(presDfm, "relFreq")
?dfm_weight
dfm_weight(presDfm, "prop")
as.matrix(dfm_weight(presDfm, "prop"))
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "prop")))
# hiarchical clustering the distance object
presCluster <- hclust(presDistMat)
# label with document names
presCluster$labels <- docnames(presDfm)
plot(presCluster)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
corpus_txt<-corpus(txt)
corpus_txt
dfm_txt<-dfm(corpus_txt)
##load in data
data("data_corpus_inaugural")
df <- texts(data_corpus_inaugural)
data_corpus_inaugural
df <- as.character(data_corpus_inaugural)
df[1]
toks <- tokens(data_corpus_inaugural)
toks
tokenz <- lengths(toks)
tokenz
# typez <- lapply(lapply(tokens,  unique ), length)
typez <- ntype(df)
df
# typez <- lapply(lapply(tokens,  unique ), length)
typez <- ntype(df)
ttr <- typez / tokenz
ttr
library(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
corpus_txt<-corpus(txt)
dfm_txt<-dfm(corpus_txt)
##load in data
data("data_corpus_inaugural")
df <- texts(data_corpus_inaugural)
df[1]
## Lexical diversity measures
# TTR
toks <- tokens(data_corpus_inaugural)
tokenz <- lengths(toks)
tokenz
# typez <- lapply(lapply(tokens,  unique ), length)
typez <- ntype(df)
ttr <- typez / tokenz
##basic plot
plot(ttr)
docvars(data_corpus_inaugural)$Party
aggregate(ttr, by = list(docvars(data_corpus_inaugural)$Party), FUN = mean)
# another way:
textstat_lexdiv(dfm(data_corpus_inaugural, groups = "President", verbose = FALSE))
library(quanteda.textstats)
# another way:
textstat_lexdiv(dfm(data_corpus_inaugural, groups = "President", verbose = FALSE))
data_corpus_inaugural
# another way:
textstat_lexdiv((data_corpus_inaugural, groups = "President", verbose = FALSE))
# another way:
textstat_lexdiv(data_corpus_inaugural, groups = "President", verbose = FALSE)
##let's look at FRE
textstat_readability(data_corpus_inaugural, "Flesch")
corr_matrix<-textstat_readability(data_corpus_inaugural, c("Flesch", "Dale.Chall", "SMOG", "Coleman.Liau" ))
corr_matrix_nums<- corr_matrix[,c(2:5)]
cor(corr_matrix_nums)
require(quanteda.textstats)
install.packages("quanteda")
library(gutenbergr)
# * Metadata for all Project Gutenberg works as R datasets, so that they can be searched and filtered:
# * `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc
# * `gutenberg_authors` contains information about each author, such as aliases and birth/death year
# * `gutenberg_subjects` contains pairings of works with Library of Congress subjects and topics
#
# ### Project Gutenberg Metadata
#
# This package contains metadata for all Project Gutenberg works as R datasets, so that you can search and filter for particular works before downloading.
#
# The dataset `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc:
install.packages("gutenbergr")
library(gutenbergr)
gutenberg_metadata
str(gutenberg_metadata)
library(dplyr)
gutenberg_metadata %>%
filter(title == "Wuthering Heights")
gutenberg_works()
?gutenberg_works
gutenberg_works(author == "Austen, Jane")
library(stringr)
gutenberg_works(str_detect(author, "Austen"))
# different languages => see list of ISO codes (https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
unique(gutenberg_metadata$language)
gutenberg_works(languages = "no") # Norwegian books
f768 <- system.file("extdata", "768.zip", package = "gutenbergr")
wuthering_heights <- gutenberg_download(768,
files = f768,
mirror = "http://aleph.gutenberg.org")
wuthering_heights <- gutenberg_download(768)
View(wuthering_heights)
?gutenberg_download
wuthering_heights <- gutenberg_download(768,
files = f768,
mirror = "http://aleph.gutenberg.org")
wuthering_heights <- gutenberg_download(768)
wuthering_heights <- gutenberg_download(768,
files = f768,
mirror = "http://aleph.gutenberg.org")
f769 <- system.file("extdata", "769.zip", package = "gutenbergr")
wuthering_heights <- gutenberg_download(768,
files = f769,
mirror = "http://aleph.gutenberg.org")
f769 <- system.file("extdata", "769.zip", package = "gutenbergr")
wuthering_heights <- gutenberg_download(768,
files = f769,
mirror = "http://aleph.gutenberg.org")
wuthering_heights <- gutenberg_download(769,
files = f769,
mirror = "http://aleph.gutenberg.org")
gutenberg_download(768,
#files = f768,
mirror = "http://aleph.gutenberg.org")
gutenberg_download(768,
#files = f768,
mirror = "http://aleph.gutenberg.org")
gutenberg_download(768)
# * Metadata for all Project Gutenberg works as R datasets, so that they can be searched and filtered:
# * `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc
# * `gutenberg_authors` contains information about each author, such as aliases and birth/death year
# * `gutenberg_subjects` contains pairings of works with Library of Congress subjects and topics
#
# ### Project Gutenberg Metadata
#
# This package contains metadata for all Project Gutenberg works as R datasets, so that you can search and filter for particular works before downloading.
#
# The dataset `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc:
install.packages("gutenbergr")
install.packages("gutenbergr")
gutenberg_metadata
library(gutenbergr)
library(gutenbergr)
gutenberg_metadata
wuthering_heights <- gutenberg_download(768)
# * Metadata for all Project Gutenberg works as R datasets, so that they can be searched and filtered:
# * `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc
# * `gutenberg_authors` contains information about each author, such as aliases and birth/death year
# * `gutenberg_subjects` contains pairings of works with Library of Congress subjects and topics
#
# ### Project Gutenberg Metadata
#
# This package contains metadata for all Project Gutenberg works as R datasets, so that you can search and filter for particular works before downloading.
#
# The dataset `gutenberg_metadata` contains information about each work, pairing Gutenberg ID with title, author, language, etc:
install.packages("curl")
wuthering_heights <- gutenberg_download(768)
wuthering_heights <- gutenberg_download(765)
my_mirror <- "http://mirrors.xmission.com/gutenberg/"
wuthering_heights <- gutenberg_download(768)
wuthering_heights <- gutenberg_download(768, mirror = my_mirror)
View(wuthering_heights)
### get the data directly from url
data_char_wh <- texts(readtext("https://www.gutenberg.org/files/768/768-0.txt"))
f1260 <- system.file("extdata", "1260.zip", package = "gutenbergr")
books <- gutenberg_download(c(768, 1260),
meta_fields = "title",
files = c(f768, f1260))
books <- gutenberg_download(c(768, 1260), meta_fields = "title")
books <- gutenberg_download(c(768, 1260), meta_fields = "title", mirror = my_mirror)
books
books %>%
count(title)
library(dplyr)
books %>%
count(title)
gutenberg_subjects
unique(gutenberg_subjects$subject)
gutenberg_subjects %>%
filter(subject == "Detective and mystery stories")
gutenberg_subjects %>%
filter(grepl("Holmes, Sherlock", subject))
gutenberg_subjects %>%
filter(str_detect(subject,"Holmes, Sherlock"))
### get the data directly from url
data_char_wh <- texts(readtext("https://www.gutenberg.org/files/768/768-0.txt"))
### create dtm/dfm
wuthering_heights <- gutenberg_download(768, mirror = my_mirror)
wuthering_heights <- wuthering_heights[!(wuthering_heights$text==""), ] # remove the empty spaces
wuthering_heights <- wuthering_heights[-c(1:2),] # remove first two rows.
wuthering_heights_corpus <- corpus(wuthering_heights) # create a corpus at line level
library(quanteda)
install.packages("quanteda")
library(quanteda)
wuthering_heights_corpus <- corpus(wuthering_heights) # create a corpus at line level
wuthering_heights_tokens <- tokens(wuthering_heights_corpus, remove_punct = TRUE)
kwic(tokens(wuthering_heights_tokens), "CHAPTER") # we can find where the chapters begin and use them to separate the text. Then perform chapter-level analyses.
dfm_wuthering_heights <- dfm(wuthering_heights_tokens, remove=stopwords("english"))
textstat_frequency(dfm_wuthering_heights,10) # find top 10 words in wuthering heights
install.packages("quanteda.textstats")
library(quanteda.textstats)
dfm_wuthering_heights <- dfm(wuthering_heights_tokens, remove=stopwords("english"))
textstat_frequency(dfm_wuthering_heights,10) # find top 10 words in wuthering heights
